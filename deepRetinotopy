#!/usr/bin/env bash

# Auto-detect number of cores (leave 1 core free)
auto_cores=$(($(nproc) - 1))
[ $auto_cores -lt 1 ] && auto_cores=1  # Ensure at least 1 core

# Default values
n_jobs=$auto_cores
subject_id=""
fast='yes'
output_dir=""
run_mode="all"  # all, step1, step2, step3

show_help() {
    cat << EOF
DeepRetinotopy: retinotopic mapping using deep learning

Usage: $(basename "$0") [OPTIONS]

Required arguments:
  -s PATH     Path to FreeSurfer subjects directory
  -t PATH     Path to HCP template surfaces directory
  -d NAME     Dataset name
  -m MAPS     Comma-separated list of maps (e.g., "polarAngle,eccentricity,pRFsize")

Optional arguments:
  -g yes/no   Fast midthickness generation (default: yes)
  -j NUM      Number of parallel jobs (default: auto-detected or 1 if single subject)
  -i ID       Process single subject ID only
  -o PATH     Output directory (default: in-place within FreeSurfer structure)

Step control (default: run complete pipeline):
  --step1     Run only Step 1: Generate midthickness surfaces and curvature maps
  --step2     Run only Step 2: Retinotopy prediction (requires Step 1 outputs)
  --step3     Run only Step 3: Resample predicted maps to native space (requires Step 1+2 outputs)

Examples:
  # Run complete pipeline (default)
  $(basename "$0") -s /data/subjects -t /data/templates -d hcp -m "polarAngle,eccentricity,pRFsize"
  
  # Only generate surfaces (useful for batch preprocessing)
  $(basename "$0") -s /data/subjects -t /data/templates -d hcp -m "polarAngle" --step1
  
  # Only run prediction (surfaces already exist from previous --step1 run)
  $(basename "$0") -s /data/subjects -t /data/templates -d hcp -m "polarAngle" --step2
  
  # Only resample to native space (Steps 1+2 already completed)
  $(basename "$0") -s /data/subjects -t /data/templates -d hcp -m "polarAngle" --step3

EOF
}

# Parse arguments
while [[ $# -gt 0 ]]; do
    case $1 in
        -s)
            dirSubs=$(realpath "$2")
            shift 2
            ;;
        -t)
            dirHCP=$(realpath "$2")
            shift 2
            ;;
        -d)
            datasetName="$2"
            shift 2
            ;;
        -m)
            IFS=',' read -ra maps <<< "$2"
            shift 2
            ;;
        -g)
            fast="$2"
            case "$fast" in
                'yes'|'no') ;;
                *) echo "Error: Invalid fast argument: $fast"; exit 1;;
            esac
            shift 2
            ;;
        -j)
            n_jobs="$2"
            shift 2
            ;;
        -i)
            subject_id="$2"
            shift 2
            ;;
        -o)
            output_dir=$(realpath "$2")
            shift 2
            ;;
        --step1)
            run_mode="step1"
            shift
            ;;
        --step2)
            run_mode="step2"
            shift
            ;;
        --step3)
            run_mode="step3"
            shift
            ;;
        -h|--help)
            show_help
            exit 0
            ;;
        *)
            echo "Error: Unknown option $1"
            show_help
            exit 1
            ;;
    esac
done

# Validate required arguments
if [ -z "$dirSubs" ] || [ -z "$dirHCP" ] || [ -z "$datasetName" ] || [ ${#maps[@]} -eq 0 ]; then
    echo "Error: Missing required arguments"
    show_help
    exit 1
fi

# Check if realpath failed
if [ ! -d "$dirSubs" ]; then
    echo "Error: Path to FreeSurfer directory ($dirSubs) does not exist."
    exit 1
fi

if [ ! -d "$dirHCP" ]; then
    echo "Error: Path to HCP surfaces directory ($dirHCP) does not exist."
    exit 1
fi

# Validate and create output directory if specified
if [ -n "$output_dir" ]; then
    mkdir -p "$output_dir"
    if [ ! -d "$output_dir" ]; then
        echo "Error: Could not create output directory ($output_dir)."
        exit 1
    fi
fi

# Validate n_jobs
if ! [[ "$n_jobs" =~ ^[0-9]+$ ]] || [ "$n_jobs" -lt 1 ]; then
    echo "Error: Invalid number of jobs ($n_jobs). Must be a positive integer."
    exit 1
fi

# Check if processing single subject or multiple subjects
if [ -n "$subject_id" ]; then
    echo "Processing single subject: $subject_id"
    # Validate subject directory exists
    if [ ! -d "$dirSubs/$subject_id" ]; then
        echo "Error: Subject directory '$subject_id' not found in $dirSubs"
        exit 1
    fi
else
    echo "Using $n_jobs parallel jobs for multiple subjects"
fi

# Determine what will run based on mode
case "$run_mode" in
    "all")
        run_description="Complete pipeline (Steps 1, 2, and 3)"
        ;;
    "step1")
        run_description="Step 1 only: Surface generation and curvature"
        ;;
    "step2")
        run_description="Step 2 only: Retinotopy prediction"
        ;;
    "step3")
        run_description="Step 3 only: Native space resampling"
        ;;
esac

# Set up logging after we have all the information
if [ -n "$output_dir" ]; then
    LOG_DIR=$output_dir
else
    LOG_DIR="$dirSubs/logs"
fi

timestamp=$(date +"%Y%m%d_%H%M%S")
log_prefix="deepRetinotopy_${timestamp}"

# Create log directory if it doesn't exist
mkdir -p "$LOG_DIR"

# Set up logging with timestamp and location info
exec > >(tee -a "$LOG_DIR/${log_prefix}_output.log")
exec 2> >(tee -a "$LOG_DIR/${log_prefix}_error.log" >&2)

echo "Log files location: $LOG_DIR"
echo "Output log: ${log_prefix}_output.log"
echo "Error log: ${log_prefix}_error.log"
echo "--------------------------------------------------------------------------------"
echo "DeepRetinotopy: retinotopic mapping using deep learning"
echo "--------------------------------------------------------------------------------"
echo "Log files: $LOG_DIR/${log_prefix}_output.log and ${log_prefix}_error.log"
echo "--------------------------------------------------------------------------------"
echo "Directories and parameters:"
echo 
echo "Path to FreeSurfer directory: $dirSubs";
echo "Path to fs_LR-deformed_to-fsaverage template surfaces: $dirHCP";
echo "Dataset name: $datasetName";
echo "Maps: ${maps[@]}";
if [ -n "$subject_id" ]; then
    echo "Processing mode: Single subject ($subject_id)";
else
    echo "Processing mode: All subjects (parallel jobs: $n_jobs)";
fi
echo "Fast midthickness generation: $fast";
if [ -n "$output_dir" ]; then
    echo "Output directory: $output_dir";
else
    echo "Output mode: In-place (within FreeSurfer directory structure)";
fi
echo "Execution mode: $run_description"
echo "--------------------------------------------------------------------------------"

if [ -d "$dirSubs" ] && [ -d "$dirHCP" ]; then
    # check if template surfaces are available (only if we're running steps that need them)
    if [ "$run_mode" = "all" ] || [ "$run_mode" = "step1" ] || [ "$run_mode" = "step3" ]; then
        if [ ! -f "$dirHCP/fs_LR-deformed_to-fsaverage.R.sphere.32k_fs_LR.surf.gii" ] && [ ! -f "$dirHCP/fs_LR-deformed_to-fsaverage.L.sphere.32k_fs_LR.surf.gii" ]; then
            echo "[Error]: Files $dirHCP/fs_LR-deformed_to-fsaverage.R.sphere.32k_fs_LR.surf.gii 
            and/or $dirHCP/fs_LR-deformed_to-fsaverage.L.sphere.32k_fs_LR.surf.gii do not exist!"
            exit 1
        fi
    fi
    
    for hemisphere in 'lh' 'rh';
    do 
        if [ $hemisphere == 'lh' ]; then
            echo "Processing left hemisphere"
        else
            echo "Processing right hemisphere"
        fi
        echo "--------------------------------------------------------------------------------"
        
        # Step 1: Generate surfaces (runs in 'all' and 'step1' modes)
        if [ "$run_mode" = "all" ] || [ "$run_mode" = "step1" ]; then
            echo "[Step 1] Generating mid-thickness surface and curvature data..."
            
            step1_args="-s $dirSubs -t $dirHCP -h $hemisphere -g $fast"
            if [ -n "$subject_id" ]; then
                step1_args="$step1_args -j 1 -i $subject_id"
            else
                step1_args="$step1_args -j $n_jobs"
            fi
            if [ -n "$output_dir" ]; then
                step1_args="$step1_args -o $output_dir"
            fi
            
            1_native2fsaverage.sh $step1_args

            if [ $? -eq 1 ]; then
                echo "Error in Step 1: Please check surface files."
                exit 1
            fi
            
            # If we're only running step1, we're done with this hemisphere
            if [ "$run_mode" = "step1" ]; then
                echo "Step 1 completed for $hemisphere hemisphere"
                continue
            fi
        fi

        # Steps 2 and 3: Process each map (only in relevant modes)
        if [ "$run_mode" = "all" ] || [ "$run_mode" = "step2" ] || [ "$run_mode" = "step3" ]; then
            for map in "${maps[@]}";
            do
                echo "--------------------------------------------------------------------------------"
                echo "Map: $map"
                echo "--------------------------------------------------------------------------------"
                
                # Step 2: Prediction (runs in 'all' and 'step2' modes)
                if [ "$run_mode" = "all" ] || [ "$run_mode" = "step2" ]; then
                    echo "[Step 2] Retinotopy prediction..."
                    
                    step2_args="--path $dirSubs --dataset $datasetName --prediction_type $map --hemisphere $hemisphere"
                    if [ -n "$subject_id" ]; then
                        step2_args="$step2_args --subject_id $subject_id"
                    fi
                    if [ -n "$output_dir" ]; then
                        step2_args="$step2_args --output_dir $output_dir"
                    fi
                    
                    2_inference.py $step2_args
                    
                    if [ $? -eq 1 ]; then
                        if [ -n "$output_dir" ]; then
                            rm -r "$output_dir/processed"
                        else
                            rm -r "$dirSubs/processed"
                        fi
                        echo "Error in Step 2: Please check if model path is correct."
                        exit 1
                    fi

                    if [ -n "$output_dir" ]; then
                        rm -r "$output_dir/processed"
                    else
                        rm -r "$dirSubs/processed"
                    fi
                    
                    # If we're only running step2, we're done with this map
                    if [ "$run_mode" = "step2" ]; then
                        echo "Step 2 completed for $map ($hemisphere hemisphere)"
                        continue
                    fi
                fi

                # Step 3: Resampling (runs in 'all' and 'step3' modes)
                if [ "$run_mode" = "all" ] || [ "$run_mode" = "step3" ]; then
                    echo "[Step 3] Resampling predictions to native space..."
                    
                    step3_args="-s $dirSubs -t $dirHCP -h $hemisphere -r $map -m model"
                    if [ -n "$subject_id" ]; then
                        step3_args="$step3_args -j 1 -i $subject_id"
                    else
                        step3_args="$step3_args -j $n_jobs"
                    fi
                    if [ -n "$output_dir" ]; then
                        step3_args="$step3_args -o $output_dir"
                    fi
                    
                    3_fsaverage2native.sh $step3_args
                    
                    if [ $? -eq 1 ]; then
                        echo "Error in Step 3: Please check if prediction files were appropriately generated."
                        exit 1
                    fi
                    
                    echo "$map resampling completed for $hemisphere hemisphere"
                fi
                echo "--------------------------------------------------------------------------------"
            done
        fi
    done
    
    echo "Execution completed!"
    if [ -n "$output_dir" ]; then
        echo "All output files have been saved to: $output_dir"
    fi
else
    echo "Directories do not exist! Please check the paths to the directories."
    exit 1
fi